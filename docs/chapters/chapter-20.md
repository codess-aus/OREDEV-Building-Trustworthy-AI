# Chapter 20: Automated Red Team Scanning

![Automated Red Teaming workflow (animated)](../images/20.%20auto%20red-teaming.gif){ .gif-figure }
<div class="gif-caption">Figure: Automated Red Teaming workflow (animated)</div>  

## Overview

So I am talking about evaluating, monitoring and managing your agents ‚Äì but one of the reasons this is so critical is because the jobs they can do now, include working side by side with our own security and compliance and SRE staff. We need to be able to trust them because they are working with us on tasks like Red Teaming ‚Äì to test the very systems they work on.

Automated scans to empower security professionals and ML engineers to proactively find risks in their generative AI systems faster with integrations of PyRIT into Azure AI Foundry.

### Scaling Red Teaming with Automation

Manual red teaming provides depth; automated red teaming provides breadth and speed:

- **Continuous Testing**: Run adversarial tests automatically on every build
- **Comprehensive Coverage**: Test thousands of attack vectors systematically
- **Regression Detection**: Catch when mitigations stop working
- **CI/CD Integration**: Block deployments that fail security thresholds
- **Cost Efficiency**: Automated testing scales without linear cost increases

### PyRIT in Azure AI Foundry

Microsoft's Python Risk Identification Toolkit (PyRIT) brings automated red teaming to Azure:

- **Built-in Attack Library**: Pre-configured jailbreak and prompt injection tests
- **Custom Scenarios**: Define application-specific adversarial tests
- **Azure Integration**: Run PyRIT scans directly in Azure AI Foundry
- **GitHub Actions**: Automate red team testing in your development pipeline

Agents can now help test the systems they work on‚Äîbut only if we can trust them. Automated red teaming with PyRIT provides the continuous adversarial testing needed to maintain that trust at scale.

## Resources and Further Reading  

- üåê [Microsoft AI Red Team](https://learn.microsoft.com/en-us/security/ai-red-team/)  
- üåê [Red Teaming](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/red-teaming)  
- üåê [AI Red Teaming Agent](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/ai-red-teaming-agent)  

## Next Steps

Continue your learning journey:

[‚Üê Chapter 19](chapter-19.md) | [Chapter 21 ‚Üí](chapter-21.md)

---

<div class="card">

**Questions or feedback?** Join the discussion on our [Building Trustworthy AI](https://github.com/codess-aus/OREDEV-Building-Trustworthy-AI) or connect with the community.

</div>
