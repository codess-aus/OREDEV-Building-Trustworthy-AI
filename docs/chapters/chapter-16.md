# Chapter 16: Manual evaluation for models and apps


![Image 16 - Manual](../images/16.%20Manual.png)  

## Overview

We recommend that you always start with manual evaluation, with human graders manually scoring generated outputs.  

When mitigating specific risks, itâ€™s really helpful to keep manually checking progress against a small dataset until evidence of the risk is no longer observed before moving on to automated evaluation.  

Azure AI Foundry provides an easy no-code interface for developers or domain experts to grade model outputs.  

## Resources and Further Reading  

### Online Resources
- ğŸŒ [Evaluate GenAI Applications](https://learn.microsoft.com/en-us/training/paths/evaluate-generative-ai-apps/)
- ğŸŒ [Evaluations in GitHub Actions](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/evaluation-github-action)  
- ğŸŒ [Evaluate generative AI models and applications by using Azure AI Foundry](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/evaluate-generative-ai-app)  
## Next Steps  

Continue your learning journey:

[â† Chapter 15](chapter-15.md) | [Chapter 17 â†’](chapter-17.md)

---

<div class="card">

**Questions or feedback?** Join the discussion on our [GitHub repository](https://github.com/codess-aus/OREDEV-Building-Trustworthy-AI) or connect with the community.

</div>
